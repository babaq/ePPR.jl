__precompile__(false)
module ePPR

import Base.length,Base.push!,Base.deleteat!,StatsBase.predict
export ePPRDebugOptions,DebugNone,DebugBasic,DebugFull,DebugVisual,
delaywindowpool,delaywindowpooloperator,delaywindowpoolblankimage,cvpartitionindex,getinitialalpha,refitmodelbetas,laplacian2dmatrix,
ePPRModel,getterm,setterm,ePPRHyperParams,ePPRCrossValidation,
eppr,epprcv,cvmodel,forwardstepwise,refitmodel,backwardstepwise,dropterm,lossfun,fitnewterm,newtontrustregion

using GLM,Roots,HypothesisTests,RCall,Dierckx
R"library('MASS')"

const DebugNone=0
const DebugBasic=1
const DebugFull=2
const DebugVisual=3
mutable struct ePPRDebugOptions
    level::Int
end
ePPRDebugOptions()=ePPRDebugOptions(DebugNone)

"""
``\hat{y}_i=\bar{y}+\sum_{d=0}^D\sum_{m=1}^{M_d}\beta_{m,d}\phi_{m,d}(\alpha_{m,d}^Tx_{i-d})``
with ``\frac{1}{n}\sum_{i=1}^n\phi_{m,d}(\alpha_{m,d}^Tx_{i-d})=0``, ``\frac{1}{n}\sum_{i=1}^n\phi_{m,d}^2(\alpha_{m,d}^Tx_{i-d})=1``
"""
mutable struct ePPRModel
    "ùë¶ÃÑ"
    ymean::Float64
    "vector of Œ≤ for each term"
    beta::Vector{Float64}
    "vector of Œ¶ for each term"
    phi::Vector
    "vector of Œ± for each term"
    alpha::Vector{Vector{Float64}}
    "vector of [temporal, spatial] index for each term"
    index::Vector{Vector{Int}}
    "vector of ``\phi_{m,d}(\alpha_{m,d}^TX_{-d})`` for each term"
    phivalues::Vector{Vector{Float64}}
    "Œ≥"
    residuals::Vector{Float64}
end
ePPRModel() = ePPRModel(0)
ePPRModel(ymean) = ePPRModel(ymean,[],[],[],[],[],[])
length(m::ePPRModel)=length(m.beta)
predict(m::ePPRModel)=m.ymean+squeeze(sum(cat(2,(m.beta.*m.phivalues)...),2),2)
predict(m::ePPRModel,x::Matrix)=m.ymean+squeeze(sum(cat(2,(m.beta.*map((Œ¶,xŒ±)->Œ¶(xŒ±),m.phi,[x].*m.alpha))...),2),2)
(m::ePPRModel)() = predict(m)
(m::ePPRModel)(x::Matrix) = predict(m,x)
function deleteat!(model::ePPRModel,i::Integer)
    deleteat!(model.beta,i)
    deleteat!(model.phi,i)
    deleteat!(model.alpha,i)
    deleteat!(model.index,i)
    deleteat!(model.phivalues,i)
end
function push!(model::ePPRModel,Œ≤::Float64,Œ¶,Œ±::Vector{Float64},index::Vector{Int},Œ¶vs::Vector{Float64})
    push!(model.beta,Œ≤)
    push!(model.phi,Œ¶)
    push!(model.alpha,Œ±)
    push!(model.index,index)
    push!(model.phivalues,Œ¶vs)
end
function getterm(model::ePPRModel,i::Integer)
    return model.beta[i],model.phi[i],model.alpha[i],model.index[i],model.phivalues[i]
end
function setterm(model::ePPRModel,i::Integer,Œ≤::Float64,Œ¶,Œ±::Vector{Float64},index::Vector{Int},Œ¶vs::Vector{Float64})
    model.beta[i]=Œ≤
    model.phi[i]=Œ¶
    model.alpha[i]=Œ±
    model.index[i]=index
    model.phivalues[i]=Œ¶vs
end

"""
Hyper Parameters for ePPR
"""
mutable struct ePPRHyperParams
    """memory size to pool for nonlinear time interaction, ndelay=1 for linear time interaction.
    only first delay terms in `nft` is used for nonlinear time interaction."""
    ndelay::Int
    "number of forward terms for each delay. [3, 2, 1] means 3 spatial terms for delay 0, 2 for delay 1, 1 for delay 2"
    nft::Vector{Int}
    "penalization parameter Œª"
    lambda::Float64
    "Œ¶ Spline degree of freedom"
    phidf::Int
    "minimum number of backward terms"
    mnbt::Int
    "whether to fit all spatial terms before moving to next temporal"
    spatialtermfirst::Bool
    "Œ± priori for penalization"
    alphapenaltyoperator
    "`(loss‚Çí-loss‚Çô)/loss‚Çí`, forward converge rate threshold to decide a saturated iteration"
    forwardconvergerate::Float64
    "`(loss‚Çí-loss‚Çô)/loss‚Çí`, refit converge rate threshold to decide a saturated iteration"
    refitconvergerate::Float64
    "number of consecutive saturated iterations to decide a solution"
    nsaturatediteration::Int
    "maximum number of iterations to fit a new term"
    newtermmaxiteration::Int
    "initial size of trust region"
    trustregioninitsize::Float64
    "maximum size of trust region"
    trustregionmaxsize::Float64
    "Œ∑ of trust region"
    trustregioneta::Float64
    "maximum iterations of trust region"
    trustregionmaxiteration::Int
    "row vector of blank image"
    blankimage
    "dimension of image"
    imagesize
    "drop term index between backward models"
    droptermindex
    "ePPR Cross Validation"
    cv
end
ePPRHyperParams()=ePPRHyperParams(1,[2,2],15,5,1,true,[],0.01,0.001,2,100,1,1000,0.2,1000,[],[],[],ePPRCrossValidation())
function ePPRHyperParams(nrow::Int,ncol::Int;ndelay::Int=1,blankcolor=0.5)
    hp=ePPRHyperParams()
    hp.imagesize = (nrow,ncol)
    hp.ndelay=ndelay
    hp.blankimage = delaywindowpoolblankimage(nrow,ncol,ndelay,blankcolor)
    hp.alphapenaltyoperator = delaywindowpooloperator(laplacian2dmatrix(nrow,ncol),ndelay)
    return hp
end
ePPRHyperParams(nrowncol::Int;ndelay::Int=1,blankcolor=0.5)=ePPRHyperParams(nrowncol,nrowncol,ndelay=ndelay,blankcolor=blankcolor)

mutable struct ePPRCrossValidation
    trainpercent::Float64
    trainfold::Int
    testfold::Int
    traintestfold::Int
    trainindex::Int
    modelselectpvalue::Float64
    trains
    tests
end
ePPRCrossValidation() = ePPRCrossValidation(0.9,5,8,8,1,0.08,[],[])

function delaywindowpool(x::Matrix,ndelay::Int,blankcolor=0.5)
    if ndelay>1
        xcol=size(x,2);dwx=x
        for j in 1:ndelay-1
            dwx = [dwx [fill(blankcolor,j,xcol);x[1:end-j,:]]]
        end
        return dwx
    end
    return x
end
delaywindowpool(x::Matrix,hp::ePPRHyperParams)=delaywindowpool(x,hp.ndelay,hp.blankimage[1])

function delaywindowpooloperator(spatialoperator::Matrix,ndelay::Int)
    if ndelay>1
        nr,nc=size(spatialoperator)
        dwo = zeros(ndelay*nr,ndelay*nc)
        for j in 0:ndelay-1
            dwo[(1:nr)+j*nr, (1:nc)+j*nc] = spatialoperator
        end
        return dwo
    end
    return spatialoperator
end

delaywindowpoolblankimage(nrow::Int,ncol::Int,ndelay::Int=1,blankcolor=0.5)=fill(blankcolor,1,nrow*ncol*ndelay)

"""
Data partition for cross validation

n: data size
cv: cross validation
"""
function cvpartitionindex(n::Int,cv::ePPRCrossValidation,debug::ePPRDebugOptions=ePPRDebugOptions())
    ntrain = cv.trainpercent*n
    ntrainfold = ntrain/cv.trainfold
    ntraintestfold = Int(floor(ntrainfold/cv.traintestfold))
    ntrainfold = Int(ntraintestfold*cv.traintestfold)
    ntrain = Int(ntrainfold*cv.trainfold)
    trains=[]
    for tf in 0:cv.trainfold-1
        traintest = Any[tf*ntrainfold + (1:ntraintestfold)+ttf*ntraintestfold for ttf in 0:cv.traintestfold-1]
        train = setdiff(1:ntrain,tf*ntrainfold + (1:ntrainfold))
        push!(trains,Any[train,traintest])
    end
    ntestfold = Int(floor((n-ntrain)/cv.testfold))
    tests = Any[ntrain + (1:ntestfold)+tf*ntestfold for tf in 0:cv.testfold-1]
    debug.level>DebugNone && println("Cross Validation Data Partition, n = $n, ntrain = $ntrain in $(cv.trainfold)-fold, ntrainfold = $ntrainfold in $(cv.traintestfold)-fold, ntest = $(ntestfold*cv.testfold) in $(cv.testfold)-fold")
    cv.trains=trains;cv.tests=tests
    return cv
end

function cvmodel(models::Vector{ePPRModel},x::Matrix,y::Vector,hp::ePPRHyperParams,debug::ePPRDebugOptions=ePPRDebugOptions())
    debug.level>DebugNone && println("ePPR Cross Validation ...")
    train = hp.cv.trains[hp.cv.trainindex][1];traintest = hp.cv.trains[hp.cv.trainindex][2]
    # response and model predication
    traintestpredications = map(m->map(i->m(x[i,:]),traintest),models)
    traintestys = map(i->y[i],traintest)
    # correlation between response and predication
    traintestcors = map(mps->cor.(traintestys,mps),traintestpredications)
    debug.level>DebugFull && display(plotcor(models,traintestcors))
    # find the model no worse than models with more terms, and better than models with less terms
    mi=0;nmodel=length(models)
    for rm in 1:nmodel
        moretermp = [pvalue(SignedRankTest(traintestcors[rm],traintestcors[m]),tail=:left) for m in rm+1:nmodel]
        if rm==1 && all(moretermp .> hp.cv.modelselectpvalue)
            mi=rm
            break
        end
        if all(moretermp .> hp.cv.modelselectpvalue)
            lesstermp = [pvalue(SignedRankTest(traintestcors[m],traintestcors[rm]),tail=:left) for m in 1:rm-1]
            if all(lesstermp .< hp.cv.modelselectpvalue)
                mi=rm
                break
            end
        end
    end
    if mi==0
        warn("No model not worse than models with more terms, and better than models with less terms")
        return nothing
    end
    model = deepcopy(models[mi])

    # find drop terms that do not improve model predication
    droptermp = [pvalue(SignedRankTest(traintestcors[m-1],traintestcors[m]),tail=:left) for m in 2:nmodel]
    notimprove = find(droptermp .> hp.cv.modelselectpvalue)
    # find models with change level predication
    modelp = [pvalue(SignedRankTest(traintestcors[m]),tail=:both) for m in 1:nmodel]
    notpredictive = find(modelp .> hp.cv.modelselectpvalue)
    # spurious terms in the selected model
    spuriousterm = findin(model.index,hp.droptermindex[union(notimprove,notpredictive)])
    !isempty(spuriousterm) && deleteat!.(model,spuriousterm)

    return eppr(model,x[train,:],y[train],hp,debug)
end

function epprcv(x::Matrix,y::Vector,hp::ePPRHyperParams,debug::ePPRDebugOptions=ePPRDebugOptions())
    n = length(y);n !=size(x,1) && error("Length of x and y does not match!")
    cvpartitionindex(n,hp.cv,debug)
    train = hp.cv.trains[hp.cv.trainindex][1]
    px = delaywindowpool(x,hp)
    models = eppr(px[train,:],y[train],hp,debug)
    return cvmodel(models,px,y,hp,debug),models
end

"""
extended Projection Pursuit Regression
by minimizing ``f=\sum_{i=1}^N(y_i-\hat{y}(x_i))^2+\lambda\sum_{d=0}^D\sum_{m=1}^{M_d}\Vert{L\alpha_{m,d}}\Vert^2``

x: matrix with one image per row
y: vector of response
hp: hyper parameters
debug: debug options
"""
function eppr(x::Matrix,y::Vector,hp::ePPRHyperParams,debug::ePPRDebugOptions=ePPRDebugOptions())
    model = forwardstepwise(x,y,hp,debug)
    model = refitmodel(model,x,y,hp,debug)
    models = backwardstepwise(model,x,y,hp,debug)
end
function eppr(model::ePPRModel,x::Matrix,y::Vector,hp::ePPRHyperParams,debug::ePPRDebugOptions=ePPRDebugOptions())
    model = forwardstepwise(model,x,y,hp,debug)
    model,model.residuals = refitmodelbetas(model,y,debug)
    return model
end

function forwardstepwise(m::ePPRModel,x::Matrix,y::Vector,hp::ePPRHyperParams,debug::ePPRDebugOptions=ePPRDebugOptions())
    js = map(t->t[1],m.index);is = map(t->t[2],m.index)
    ujs = unique(js);jis=Dict(j=>is[js.==j] for j in ujs);njis=Dict(j=>length(jis[j]) for j in ujs)
    debug.level>DebugNone && println("ePPR Forward Stepwise ...")
    ym = mean(y);model = ePPRModel(ym);r=y-ym
    if hp.spatialtermfirst
        for j in ujs
            tx = j>0?[repmat(hp.blankimage,j);x[1:end-j,:]]:x
            for i in 1:njis[j]
                debug.level>DebugNone && println("Fit [Temporal-$j, Spatial-$i] New Term ...")
                Œ± = normalize(m.alpha[ m.index .== [[j,jis[j][i]]] ][1],2)
                Œ≤,Œ¶,Œ±,Œ¶vs = fitnewterm(tx,r,Œ±,hp.phidf,debug)
                r -= Œ≤*Œ¶vs
                push!(model,Œ≤,Œ¶,Œ±,[j,i],Œ¶vs)
            end
        end
    else
        for i in 1:maximum(values(njis)),j in ujs
            i>njis[j] && continue
            debug.level>DebugNone && println("Fit [Temporal-$j, Spatial-$i] New Term ...")
            tx = j>0?[repmat(hp.blankimage,j);x[1:end-j,:]]:x
            Œ± = normalize(m.alpha[ m.index .== [[j,jis[j][i]]] ][1],2)
            Œ≤,Œ¶,Œ±,Œ¶vs = fitnewterm(tx,r,Œ±,hp.phidf,debug)
            r -= Œ≤*Œ¶vs
            push!(model,Œ≤,Œ¶,Œ±,[j,i],Œ¶vs)
        end
    end
    model.residuals=r
    return model
end

function forwardstepwise(x::Matrix,y::Vector,hp::ePPRHyperParams,debug::ePPRDebugOptions=ePPRDebugOptions())
    debug.level>DebugNone && println("ePPR Forward Stepwise ...")
    if hp.ndelay>1
        hp.nft=hp.nft[1:1]
    end
    ym = mean(y);model = ePPRModel(ym);r=y-ym
    if hp.spatialtermfirst
        for j in 0:length(hp.nft)-1
            tx = j>0?[repmat(hp.blankimage,j);x[1:end-j,:]]:x
            for i in 1:hp.nft[j+1]
                debug.level>DebugNone && println("Fit [Temporal-$j, Spatial-$i] New Term ...")
                Œ± = getinitialalpha(tx,r,debug)
                Œ≤,Œ¶,Œ±,Œ¶vs = fitnewterm(tx,r,Œ±,hp,debug)
                r -= Œ≤*Œ¶vs
                push!(model,Œ≤,Œ¶,Œ±,[j,i],Œ¶vs)
            end
        end
    else
        for i in 1:maximum(hp.nft),j in 0:length(hp.nft)-1
            i>hp.nft[j+1] && continue
            debug.level>DebugNone && println("Fit [Temporal-$j, Spatial-$i] New Term ...")
            tx = j>0?[repmat(hp.blankimage,j);x[1:end-j,:]]:x
            Œ± = getinitialalpha(tx,r,debug)
            Œ≤,Œ¶,Œ±,Œ¶vs = fitnewterm(tx,r,Œ±,hp,debug)
            r -= Œ≤*Œ¶vs
            push!(model,Œ≤,Œ¶,Œ±,[j,i],Œ¶vs)
        end
    end
    model.residuals=r
    return model
end

function refitmodel(model::ePPRModel,x::Matrix,y::Vector,hp::ePPRHyperParams,debug::ePPRDebugOptions=ePPRDebugOptions())
    debug.level>DebugNone && println("ePPR Model Refit ...")
    model,r = refitmodelbetas(model,y,debug)
    for t in 1:length(model)
        oldloss = lossfun(model,y,hp)
        oldŒ≤,oldŒ¶,oldŒ±,index,oldŒ¶vs = getterm(model,t)
        r += oldŒ≤*oldŒ¶vs

        j = index[1];i=index[2]
        tx = j>0?[repmat(hp.blankimage,j);x[1:end-j,:]]:x
        debug.level>DebugNone && println("Refit [Temporal-$j, Spatial-$i] New Term ...")
        Œ≤,Œ¶,Œ±,Œ¶vs = fitnewterm(tx,r,oldŒ±,hp,debug,forward=false)
        setterm(model,t,Œ≤,Œ¶,Œ±,index,Œ¶vs)
        newloss = lossfun(model,y,hp)
        if newloss > oldloss
            debug.level>DebugNone && println("Model Loss increased from $oldloss to $newloss. Discard the new term, keep the old one.")
            setterm(model,t,oldŒ≤,oldŒ¶,oldŒ±,index,oldŒ¶vs)
            r -= oldŒ≤*oldŒ¶vs
        else
            r -= Œ≤*Œ¶vs
        end
    end
    model.residuals=r
    return model
end

function backwardstepwise(model::ePPRModel,x::Matrix,y::Vector,hp::ePPRHyperParams,debug::ePPRDebugOptions=ePPRDebugOptions())
    debug.level>DebugNone && println("ePPR Backward Stepwise ...")
    if debug.level>DebugFull
        display(plotalpha(model,hp))
        display(plotphi(model))
    end
    models=[deepcopy(model)];droptermindex=[]
    for i in length(model):-1:hp.mnbt+1
        model,index = dropleastimportantterm(model,debug)
        unshift!(droptermindex,index)
        model = refitmodel(model,x,y,hp,debug)
        if debug.level>DebugFull
            display(plotalpha(model,hp))
            display(plotphi(model))
        end
        unshift!(models,deepcopy(model))
    end
    hp.droptermindex = droptermindex
    return models
end

dropleastimportantterm(model::ePPRModel,debug::ePPRDebugOptions=ePPRDebugOptions())=dropterm(model,indmin(abs.(model.beta)),debug)

function dropterm(model::ePPRModel,i::Integer,debug::ePPRDebugOptions=ePPRDebugOptions())
    index = model.index[i]
    Œ≤=model.beta[i]
    debug.level>DebugNone && println("Drop Term: [temporal-$(index[1]), spatial-$(index[2])] with Œ≤: $(Œ≤).")
    deleteat!(model,i)
    return model,index
end

lossfun(g::Vector) = 0.5*norm(g,2)^2
"""
Loss function for term
f(Œ±) = sum((r-Œ¶(x*Œ±)).^2) + Œª*norm(hp.alphapenaltyoperator*Œ±,2)^2
"""
lossfun(r::Vector,x::Matrix,Œ±::Vector,Œ¶,hp::ePPRHyperParams) = lossfun([r-Œ¶(x*Œ±);sqrt(hp.lambda)*hp.alphapenaltyoperator*Œ±])
"Loss function for model"
function lossfun(model::ePPRModel,y::Vector,hp::ePPRHyperParams)
    modelloss = lossfun(y-predict(model))
    penaltyloss = 0.5*hp.lambda*sum(norm.([hp.alphapenaltyoperator].*model.alpha,2).^2)
    return modelloss + penaltyloss
end

(Œ¶ro::RObject)(xŒ±) = rcopy(R"predict($Œ¶ro, x=$xŒ±)$y")
function fitnewterm(x::Matrix,r::Vector,Œ±::Vector,hp::ePPRHyperParams,debug::ePPRDebugOptions=ePPRDebugOptions();forward::Bool=true)
    saturatediteration = 0;Œ¶=nothing;Œ¶vs=nothing;xŒ±=nothing;crt = forward?hp.forwardconvergerate:hp.refitconvergerate
    for i in 1:hp.newtermmaxiteration
        xŒ± = x*Œ±
        Œ¶ = R"smooth.spline(y=$r, x=$xŒ±, df=$(hp.phidf), spar=NULL, cv=FALSE)"
        Œ¶vs = Œ¶(xŒ±)
        f(a) = lossfun(r,x,a,Œ¶,hp)
        gt = r-Œ¶vs;gp = sqrt(hp.lambda)*hp.alphapenaltyoperator*Œ±;g = [gt;gp]
        debug.level>DebugNone && println("New Term $(i)th iteration. TermLoss: $(lossfun(gt)), PenaltyLoss: $(lossfun(gp)).")
        # Loss f(Œ±) before trust region
        loss‚Çí = lossfun(g)
        Œ¶‚Ä≤vs = rcopy(R"predict($Œ¶, x=$xŒ±, deriv=1)$y")
        gg = [-Œ¶‚Ä≤vs.*x;sqrt(hp.lambda)*hp.alphapenaltyoperator]'
        f‚Ä≤ = gg*g
        f‚Ä≥ = gg*gg'
        # Œ± and Loss f(Œ±) after trust region
        success,Œ±,loss‚Çô = newtontrustregion(f,Œ±,loss‚Çí,f‚Ä≤,f‚Ä≥,hp.trustregioninitsize,hp.trustregionmaxsize,hp.trustregioneta,hp.trustregionmaxiteration,debug)
        if !success
            warn("NewtonTrustRegion failed, New Term use initial Œ±.")
            break
        end
        loss‚Çô > loss‚Çí && debug.level>DebugNone && println("New Term $(i)th iteration. Loss increased from $(loss‚Çí) to $(loss‚Çô).")
        cr = (loss‚Çí-loss‚Çô)/loss‚Çí
        if loss‚Çô < loss‚Çí && cr < crt
            saturatediteration+=1
            if saturatediteration >= hp.nsaturatediteration
                debug.level>DebugNone && println("New Term converged in $i iterations with (loss‚Çí-loss‚Çô)/loss‚Çí = $(cr).")
                break
            end
        else
            saturatediteration=0
        end
        i==hp.newtermmaxiteration && warn("New Term does not converge in $i iterations.")
    end
    Œ≤ = std(Œ¶vs)
    Œ¶vs /=Œ≤
    si = sortperm(xŒ±)
    Œ¶ = Spline1D(xŒ±[si], Œ¶(xŒ±[si]), k=3, bc="extrapolate", s=50)
    return Œ≤,Œ¶,Œ±,Œ¶vs
end

function fitnewterm(x::Matrix,r::Vector,Œ±::Vector,phidf::Int,debug::ePPRDebugOptions=ePPRDebugOptions())
    xŒ± = x*Œ±
    Œ¶ = R"smooth.spline(y=$r, x=$xŒ±, df=$phidf, spar=NULL, cv=FALSE)"
    Œ¶vs = Œ¶(xŒ±)
    Œ≤ = std(Œ¶vs)
    Œ¶vs /=Œ≤
    si = sortperm(xŒ±)
    Œ¶ = Spline1D(xŒ±[si], Œ¶(xŒ±[si]), k=3, bc="extrapolate", s=50)
    return Œ≤,Œ¶,Œ±,Œ¶vs
end

"""
Update Œ± only once
"Numerical Optimization, Nocedal and Wright, 2006"

subproblem: Min m·µ¢(p) = f·µ¢ + g·µ¢·µÄp + 0.5p·µÄB·µ¢p , ‚à•p‚à• ‚©Ω r·µ¢

Theorem 4.1
a: (B·µ¢ + ŒªI)pÀ¢ = -g·µ¢ , Œª ‚©æ 0
b: Œª(r·µ¢ - ‚à•pÀ¢‚à•) = 0
c: B·µ¢ + ŒªI positive definite

Œª = 0 => ‚à•pÀ¢‚à• ‚©Ω r·µ¢, B·µ¢pÀ¢ = -g·µ¢, B·µ¢ positive definite
‚à•pÀ¢‚à• = r·µ¢ => Œª ‚©æ 0, p(Œª) = -(B·µ¢ + ŒªI)‚Åª¬πg·µ¢, B·µ¢ + ŒªI positive definite
"""
function newtontrustregion(f::Function,x‚ÇÄ::Vector,f‚ÇÄ::Float64,g‚ÇÄ::Vector,H‚ÇÄ::Matrix,r::Float64,rmax::Float64,Œ∑::Float64,maxiteration::Int,debug::ePPRDebugOptions)
    eh = eigfact(Symmetric(H‚ÇÄ))
    posdef = isposdef(eh)
    q·µÄg = eh[:vectors]'*g‚ÇÄ
    if posdef
        pÀ¢ = -eh[:vectors]*(q·µÄg./eh[:values])
        pÀ¢‚Çô = norm(pÀ¢,2)
    end
    Œªe = eh[:values]-eh[:values][1]
    C1 = sum((q·µÄg./Œªe)[2:end].^2)
    C2 = q·µÄg[1]^2
    C3 = sum(q·µÄg.^2)

    for i in 1:maxiteration
        debug.level>DebugBasic && println("NewtonTrustRegion $(i)th iteration, r = $(r)")
        # try for solution when Œª = 0
        if posdef && pÀ¢‚Çô <= r
            p·µ¢ = pÀ¢
            islambdazero = true
        else
            islambdazero = false
            # easy or hard-easy cases
            if C2 > 0 || C1 >= r^2
                iseasy = true
                ishard = C2==0

                Œªdn = sqrt(C2)/r
                Œªup = sqrt(C3)/r
                function œï(Œª)
                    if Œª==0
                        if C2 > 0
                            return -1/r
                        else
                            return sqrt(1/C1) - 1/r
                        end
                    end
                    return 1/norm(q·µÄg./(Œªe+Œª),2) - 1/r
                end
                if œï(Œªup) <= 0
                    Œª = Œªup
                elseif œï(Œªdn) >= 0
                    Œª = Œªdn
                else
                    Œª = fzero(œï,Œªdn,Œªup)
                end
                p·µ¢ = -eh[:vectors]*(q·µÄg./(Œªe+Œª))
            else
                iseasy = false
                ishard = true
                # hard-hard case
                w = q·µÄg./Œªe
                w[1]=0
                œÑ = sqrt(r^2-C1)
                ùëß = eh[:vectors][:,1]
                p·µ¢ = -eh[:vectors]*w + œÑ*ùëß
            end
        end
        # œÅ: ratio of actual change versus predicted change
        x·µ¢ = x‚ÇÄ + p·µ¢
        f·µ¢ = f(x·µ¢)
        œÅ = (f·µ¢ - f‚ÇÄ) / (p·µ¢'*g‚ÇÄ + p·µ¢'*H‚ÇÄ*p·µ¢/2)
        # update trust region size
        if œÅ < 0.25
            r /= 4
        elseif œÅ > 0.75 && !islambdazero
            r = min(2r,rmax)
        end
        if debug.level>DebugBasic
            println("                                 œÅ = $œÅ")
            if islambdazero
                steptype="Œª = 0"
            else
                if ishard
                    steptype=iseasy?"hard-easy":"hard-hard"
                else
                    steptype="easy"
                end
            end
            println("                                 step is $steptype")
        end
        # accept solution only once
        if œÅ > Œ∑
            return true,x·µ¢,f·µ¢
        end
    end
    debug.level>DebugBasic && warn("NewtonTrustRegion does not converge in $maxiteration iterations.")
    return false,x‚ÇÄ,f‚ÇÄ
end

function getinitialalpha(x::Matrix,r::Vector,debug::ePPRDebugOptions=ePPRDebugOptions())
    debug.level>DebugNone && println("Get Initial Œ± ...")
    # RCall lm.ridge with kLW lambda
    Œ± = rcopy(R"""
    lmr = lm.ridge($r ~ 0 + $x)
    lmr = lm.ridge($r ~ 0 + $x, lambda=lmr$kLW)
    coefficients(lmr)
    """)
    Œ±-=mean(Œ±);normalize!(Œ±,2);Œ±
end

function refitmodelbetas(model::ePPRModel,y::Vector,debug::ePPRDebugOptions=ePPRDebugOptions())
    debug.level>DebugNone && println("Refit Œ≤s ...")
    x = cat(2,model.phivalues...)
    res = lm(x,y-model.ymean)
    Œ≤ = coef(res)
    if debug.level>DebugNone
        println("Old Œ≤s: $(model.beta)")
        println("New Œ≤s: $Œ≤")
    end
    model.beta = Œ≤
    return model,residuals(res)
end

function laplacian2dmatrix(nrow::Int,ncol::Int)
    center = [-1 -1 -1;
              -1  8 -1;
              -1 -1 -1]
    firstrow=[-1  5 -1;
              -1 -1 -1;
               0  0  0]
    lastrow= [ 0  0  0;
              -1 -1 -1;
              -1  5 -1]
    firstcol=[-1 -1  0;
               5 -1  0;
              -1 -1  0]
    lastcol= [ 0 -1 -1;
               0 -1  5;
               0 -1 -1]
    topleft= [ 3 -1  0;
              -1 -1  0;
               0  0  0]
    topright=[ 0 -1  3;
               0 -1 -1;
               0  0  0]
    downleft=[ 0  0  0;
              -1 -1  0;
               3 -1  0]
    downright=[0  0  0;
               0 -1 -1;
               0 -1  3]
    ldim=(nrow,ncol)
    lm = zeros(nrow*ncol,nrow*ncol)
    # fill center
    for r in 2:nrow-1, c in 2:ncol-1
        f=zeros(nrow,ncol)
        f[r-1:r+1,c-1:c+1]=center
        lm[sub2ind(ldim,r,c),:]=vec(f)
    end
    for c in 2:ncol-1
        # fill first row
        r=1;f=zeros(nrow,ncol)
        f[r:r+2,c-1:c+1]=firstrow
        lm[sub2ind(ldim,r,c),:]=vec(f)
        # fill last row
        r=nrow;f=zeros(nrow,ncol)
        f[r-2:r,c-1:c+1]=lastrow
        lm[sub2ind(ldim,r,c),:]=vec(f)
    end
    for r in 2:nrow-1
        # fill first col
        c=1;f=zeros(nrow,ncol)
        f[r-1:r+1,c:c+2]=firstcol
        lm[sub2ind(ldim,r,c),:]=vec(f)
        # fill last col
        c=ncol;f=zeros(nrow,ncol)
        f[r-1:r+1,c-2:c]=lastcol
        lm[sub2ind(ldim,r,c),:]=vec(f)
    end
    # fill top left
    r=1;c=1;f=zeros(nrow,ncol)
    f[r:r+2,c:c+2]=topleft
    lm[sub2ind(ldim,r,c),:]=vec(f)
    # fill top right
    r=1;c=ncol;f=zeros(nrow,ncol)
    f[r:r+2,c-2:c]=topright
    lm[sub2ind(ldim,r,c),:]=vec(f)
    # fill down left
    r=nrow;c=1;f=zeros(nrow,ncol)
    f[r-2:r,c:c+2]=downleft
    lm[sub2ind(ldim,r,c),:]=vec(f)
    # fill down right
    r=nrow;c=ncol;f=zeros(nrow,ncol)
    f[r-2:r,c-2:c]=downright
    lm[sub2ind(ldim,r,c),:]=vec(f)
    return lm
end

include("Visualization.jl")
end # module
